{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import multiprocessing\n",
    "from IMDBDataset import IMDBDataset, IMDBCollate\n",
    "import time\n",
    "\n",
    "import spacy\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "# split the dataset into train, test and validation sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(train_df, tokenizer, min_freq=5, vocabulary=None)\n",
    "val_dataset = IMDBDataset(val_df, tokenizer, min_freq=None, vocabulary=train_dataset.vocab)\n",
    "test_dataset = IMDBDataset(test_df, tokenizer, min_freq=None, vocabulary=train_dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=32, \n",
    "                              shuffle=True,\n",
    "                              collate_fn=IMDBCollate(pad_idx=train_dataset.vocab['<PAD>']), \n",
    "                              num_workers=multiprocessing.cpu_count())\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=32, \n",
    "                            shuffle=False, \n",
    "                            collate_fn=IMDBCollate(pad_idx=train_dataset.vocab['<PAD>']), \n",
    "                            num_workers=multiprocessing.cpu_count())\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=32, \n",
    "                             shuffle=False, \n",
    "                             collate_fn=IMDBCollate(pad_idx=train_dataset.vocab['<PAD>']), \n",
    "                             num_workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = train_dataset.vocab.__len__()\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # self.rnn = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text) # text = [batch size, sentence length] -> embedded = [batch size, sentence length, embedding dim]\n",
    "        # output, (hidden, cell) = self.rnn(embedded) # output = [batch size, sent len, hid dim], hidden = [num_layer=1, batch size, hid dim], cell = [num_layer=1, batch size, hid dim]\n",
    "        output, hidden = self.rnn(embedded) # output = [batch size, sent len, hid dim], hidden = [num_layer=1, batch size, hid dim], cell = [num_layer=1, batch size, hid dim]\n",
    "        return self.fc(hidden.squeeze(0)) # hidden = [num_layer=1, batch size, hid dim] -> hidden.squeeze(0) = [batch size, hid dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = torch.nn.Embedding(VOCABULARY_SIZE, EMBEDDING_DIM)\n",
    "# rnn = torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     text, label = batch\n",
    "#     embedded = embedding(text)\n",
    "#     output, (hidden, cell) = rnn(embedded)\n",
    "#     print('text shape: ', text.shape)\n",
    "#     print('embedded shape: ', embedded.shape)\n",
    "#     print('output shape: ', output.shape)\n",
    "#     print('hidden shape: ', hidden.shape)\n",
    "#     print('cell shape: ', cell.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_dim=VOCABULARY_SIZE,\n",
    "            embedding_dim=EMBEDDING_DIM, \n",
    "            hidden_dim=HIDDEN_DIM, \n",
    "            output_dim=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5601538"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/1125 | Loss: 0.7213\n",
      "Epoch: 001/015 | Batch 050/1125 | Loss: 0.7217\n",
      "Epoch: 001/015 | Batch 100/1125 | Loss: 0.6919\n",
      "Epoch: 001/015 | Batch 150/1125 | Loss: 0.6629\n",
      "Epoch: 001/015 | Batch 200/1125 | Loss: 0.6295\n",
      "Epoch: 001/015 | Batch 250/1125 | Loss: 0.8206\n",
      "Epoch: 001/015 | Batch 300/1125 | Loss: 0.7054\n",
      "Epoch: 001/015 | Batch 350/1125 | Loss: 0.6780\n",
      "Epoch: 001/015 | Batch 400/1125 | Loss: 0.7195\n",
      "Epoch: 001/015 | Batch 450/1125 | Loss: 0.7121\n",
      "Epoch: 001/015 | Batch 500/1125 | Loss: 0.7643\n",
      "Epoch: 001/015 | Batch 550/1125 | Loss: 0.7188\n",
      "Epoch: 001/015 | Batch 600/1125 | Loss: 0.8829\n",
      "Epoch: 001/015 | Batch 650/1125 | Loss: 0.7059\n",
      "Epoch: 001/015 | Batch 700/1125 | Loss: 0.6057\n",
      "Epoch: 001/015 | Batch 750/1125 | Loss: 0.6217\n",
      "Epoch: 001/015 | Batch 800/1125 | Loss: 0.7067\n",
      "Epoch: 001/015 | Batch 850/1125 | Loss: 0.7004\n",
      "Epoch: 001/015 | Batch 900/1125 | Loss: 0.6920\n",
      "Epoch: 001/015 | Batch 950/1125 | Loss: 0.7005\n",
      "Epoch: 001/015 | Batch 1000/1125 | Loss: 0.7276\n",
      "Epoch: 001/015 | Batch 1050/1125 | Loss: 0.6926\n",
      "Epoch: 001/015 | Batch 1100/1125 | Loss: 0.7198\n",
      "training accuracy: 50.01%\n",
      "valid accuracy: 49.10%\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 002/015 | Batch 000/1125 | Loss: 0.6907\n",
      "Epoch: 002/015 | Batch 050/1125 | Loss: 0.6912\n",
      "Epoch: 002/015 | Batch 100/1125 | Loss: 0.7337\n",
      "Epoch: 002/015 | Batch 150/1125 | Loss: 0.7667\n",
      "Epoch: 002/015 | Batch 200/1125 | Loss: 0.7463\n",
      "Epoch: 002/015 | Batch 250/1125 | Loss: 0.7797\n",
      "Epoch: 002/015 | Batch 300/1125 | Loss: 0.6743\n",
      "Epoch: 002/015 | Batch 350/1125 | Loss: 0.6587\n",
      "Epoch: 002/015 | Batch 400/1125 | Loss: 0.7141\n",
      "Epoch: 002/015 | Batch 450/1125 | Loss: 0.7056\n",
      "Epoch: 002/015 | Batch 500/1125 | Loss: 0.6667\n",
      "Epoch: 002/015 | Batch 550/1125 | Loss: 0.7004\n",
      "Epoch: 002/015 | Batch 600/1125 | Loss: 0.7435\n",
      "Epoch: 002/015 | Batch 650/1125 | Loss: 0.9371\n",
      "Epoch: 002/015 | Batch 700/1125 | Loss: 0.6889\n",
      "Epoch: 002/015 | Batch 750/1125 | Loss: 0.6801\n",
      "Epoch: 002/015 | Batch 800/1125 | Loss: 0.7988\n",
      "Epoch: 002/015 | Batch 850/1125 | Loss: 0.7709\n",
      "Epoch: 002/015 | Batch 900/1125 | Loss: 0.8767\n",
      "Epoch: 002/015 | Batch 950/1125 | Loss: 0.6905\n",
      "Epoch: 002/015 | Batch 1000/1125 | Loss: 0.7728\n",
      "Epoch: 002/015 | Batch 1050/1125 | Loss: 0.6784\n",
      "Epoch: 002/015 | Batch 1100/1125 | Loss: 0.6440\n",
      "training accuracy: 50.24%\n",
      "valid accuracy: 49.05%\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 003/015 | Batch 000/1125 | Loss: 0.6858\n",
      "Epoch: 003/015 | Batch 050/1125 | Loss: 0.6957\n",
      "Epoch: 003/015 | Batch 100/1125 | Loss: 0.6945\n",
      "Epoch: 003/015 | Batch 150/1125 | Loss: 0.6920\n",
      "Epoch: 003/015 | Batch 200/1125 | Loss: 0.6430\n",
      "Epoch: 003/015 | Batch 250/1125 | Loss: 0.6993\n",
      "Epoch: 003/015 | Batch 300/1125 | Loss: 0.7192\n",
      "Epoch: 003/015 | Batch 350/1125 | Loss: 0.7856\n",
      "Epoch: 003/015 | Batch 400/1125 | Loss: 0.7208\n",
      "Epoch: 003/015 | Batch 450/1125 | Loss: 0.7072\n",
      "Epoch: 003/015 | Batch 500/1125 | Loss: 0.6698\n",
      "Epoch: 003/015 | Batch 550/1125 | Loss: 0.7874\n",
      "Epoch: 003/015 | Batch 600/1125 | Loss: 0.6840\n",
      "Epoch: 003/015 | Batch 650/1125 | Loss: 0.6578\n",
      "Epoch: 003/015 | Batch 700/1125 | Loss: 0.6736\n",
      "Epoch: 003/015 | Batch 750/1125 | Loss: 0.7697\n",
      "Epoch: 003/015 | Batch 800/1125 | Loss: 0.6791\n",
      "Epoch: 003/015 | Batch 850/1125 | Loss: 0.7143\n",
      "Epoch: 003/015 | Batch 900/1125 | Loss: 0.6884\n",
      "Epoch: 003/015 | Batch 950/1125 | Loss: 0.6792\n",
      "Epoch: 003/015 | Batch 1000/1125 | Loss: 0.6729\n",
      "Epoch: 003/015 | Batch 1050/1125 | Loss: 0.7543\n",
      "Epoch: 003/015 | Batch 1100/1125 | Loss: 0.7635\n",
      "training accuracy: 50.37%\n",
      "valid accuracy: 49.08%\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 004/015 | Batch 000/1125 | Loss: 0.7180\n",
      "Epoch: 004/015 | Batch 050/1125 | Loss: 0.8116\n",
      "Epoch: 004/015 | Batch 100/1125 | Loss: 0.7021\n",
      "Epoch: 004/015 | Batch 150/1125 | Loss: 0.6986\n",
      "Epoch: 004/015 | Batch 200/1125 | Loss: 0.7352\n",
      "Epoch: 004/015 | Batch 250/1125 | Loss: 0.7595\n",
      "Epoch: 004/015 | Batch 300/1125 | Loss: 0.9021\n",
      "Epoch: 004/015 | Batch 350/1125 | Loss: 0.7374\n",
      "Epoch: 004/015 | Batch 400/1125 | Loss: 0.6743\n",
      "Epoch: 004/015 | Batch 450/1125 | Loss: 0.7107\n",
      "Epoch: 004/015 | Batch 500/1125 | Loss: 0.6866\n",
      "Epoch: 004/015 | Batch 550/1125 | Loss: 0.6958\n",
      "Epoch: 004/015 | Batch 600/1125 | Loss: 0.7731\n",
      "Epoch: 004/015 | Batch 650/1125 | Loss: 0.8012\n",
      "Epoch: 004/015 | Batch 700/1125 | Loss: 0.9798\n",
      "Epoch: 004/015 | Batch 750/1125 | Loss: 0.8260\n",
      "Epoch: 004/015 | Batch 800/1125 | Loss: 0.6772\n",
      "Epoch: 004/015 | Batch 850/1125 | Loss: 0.6883\n",
      "Epoch: 004/015 | Batch 900/1125 | Loss: 0.6577\n",
      "Epoch: 004/015 | Batch 950/1125 | Loss: 0.7276\n",
      "Epoch: 004/015 | Batch 1000/1125 | Loss: 0.8143\n",
      "Epoch: 004/015 | Batch 1050/1125 | Loss: 0.6905\n",
      "Epoch: 004/015 | Batch 1100/1125 | Loss: 0.7450\n",
      "training accuracy: 50.00%\n",
      "valid accuracy: 50.90%\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 005/015 | Batch 000/1125 | Loss: 0.6691\n",
      "Epoch: 005/015 | Batch 050/1125 | Loss: 0.6793\n",
      "Epoch: 005/015 | Batch 100/1125 | Loss: 0.6916\n",
      "Epoch: 005/015 | Batch 150/1125 | Loss: 0.8300\n",
      "Epoch: 005/015 | Batch 200/1125 | Loss: 0.6694\n",
      "Epoch: 005/015 | Batch 250/1125 | Loss: 0.7150\n",
      "Epoch: 005/015 | Batch 300/1125 | Loss: 0.7021\n",
      "Epoch: 005/015 | Batch 350/1125 | Loss: 0.7008\n",
      "Epoch: 005/015 | Batch 400/1125 | Loss: 0.7673\n",
      "Epoch: 005/015 | Batch 450/1125 | Loss: 0.7321\n",
      "Epoch: 005/015 | Batch 500/1125 | Loss: 0.8248\n",
      "Epoch: 005/015 | Batch 550/1125 | Loss: 0.7561\n",
      "Epoch: 005/015 | Batch 600/1125 | Loss: 0.7540\n",
      "Epoch: 005/015 | Batch 650/1125 | Loss: 0.6755\n",
      "Epoch: 005/015 | Batch 700/1125 | Loss: 0.6807\n",
      "Epoch: 005/015 | Batch 750/1125 | Loss: 0.6703\n",
      "Epoch: 005/015 | Batch 800/1125 | Loss: 0.6786\n",
      "Epoch: 005/015 | Batch 850/1125 | Loss: 0.7000\n",
      "Epoch: 005/015 | Batch 900/1125 | Loss: 0.7059\n",
      "Epoch: 005/015 | Batch 950/1125 | Loss: 0.6894\n",
      "Epoch: 005/015 | Batch 1000/1125 | Loss: 0.7427\n",
      "Epoch: 005/015 | Batch 1050/1125 | Loss: 0.6940\n",
      "Epoch: 005/015 | Batch 1100/1125 | Loss: 0.7660\n",
      "training accuracy: 50.08%\n",
      "valid accuracy: 49.20%\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 006/015 | Batch 000/1125 | Loss: 0.7103\n",
      "Epoch: 006/015 | Batch 050/1125 | Loss: 0.7996\n",
      "Epoch: 006/015 | Batch 100/1125 | Loss: 0.7426\n",
      "Epoch: 006/015 | Batch 150/1125 | Loss: 0.8338\n",
      "Epoch: 006/015 | Batch 200/1125 | Loss: 0.8275\n",
      "Epoch: 006/015 | Batch 250/1125 | Loss: 0.6765\n",
      "Epoch: 006/015 | Batch 300/1125 | Loss: 0.8504\n",
      "Epoch: 006/015 | Batch 350/1125 | Loss: 0.7371\n",
      "Epoch: 006/015 | Batch 400/1125 | Loss: 0.7377\n",
      "Epoch: 006/015 | Batch 450/1125 | Loss: 0.6746\n",
      "Epoch: 006/015 | Batch 500/1125 | Loss: 0.7915\n",
      "Epoch: 006/015 | Batch 550/1125 | Loss: 0.7369\n",
      "Epoch: 006/015 | Batch 600/1125 | Loss: 0.7227\n",
      "Epoch: 006/015 | Batch 650/1125 | Loss: 0.6709\n",
      "Epoch: 006/015 | Batch 700/1125 | Loss: 0.7566\n",
      "Epoch: 006/015 | Batch 750/1125 | Loss: 0.6932\n",
      "Epoch: 006/015 | Batch 800/1125 | Loss: 0.6906\n",
      "Epoch: 006/015 | Batch 850/1125 | Loss: 0.7515\n",
      "Epoch: 006/015 | Batch 900/1125 | Loss: 0.6999\n",
      "Epoch: 006/015 | Batch 950/1125 | Loss: 0.7224\n",
      "Epoch: 006/015 | Batch 1000/1125 | Loss: 0.6851\n",
      "Epoch: 006/015 | Batch 1050/1125 | Loss: 0.6815\n",
      "Epoch: 006/015 | Batch 1100/1125 | Loss: 0.7142\n",
      "training accuracy: 50.01%\n",
      "valid accuracy: 49.15%\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 007/015 | Batch 000/1125 | Loss: 0.7848\n",
      "Epoch: 007/015 | Batch 050/1125 | Loss: 0.6858\n",
      "Epoch: 007/015 | Batch 100/1125 | Loss: 0.7924\n",
      "Epoch: 007/015 | Batch 150/1125 | Loss: 0.7521\n",
      "Epoch: 007/015 | Batch 200/1125 | Loss: 0.7118\n",
      "Epoch: 007/015 | Batch 250/1125 | Loss: 0.6905\n",
      "Epoch: 007/015 | Batch 300/1125 | Loss: 0.5929\n",
      "Epoch: 007/015 | Batch 350/1125 | Loss: 0.7178\n",
      "Epoch: 007/015 | Batch 400/1125 | Loss: 0.7481\n",
      "Epoch: 007/015 | Batch 450/1125 | Loss: 0.6774\n",
      "Epoch: 007/015 | Batch 500/1125 | Loss: 0.7787\n",
      "Epoch: 007/015 | Batch 550/1125 | Loss: 0.6844\n",
      "Epoch: 007/015 | Batch 600/1125 | Loss: 0.6731\n",
      "Epoch: 007/015 | Batch 650/1125 | Loss: 0.7216\n",
      "Epoch: 007/015 | Batch 700/1125 | Loss: 0.7521\n",
      "Epoch: 007/015 | Batch 750/1125 | Loss: 0.6867\n",
      "Epoch: 007/015 | Batch 800/1125 | Loss: 0.7321\n",
      "Epoch: 007/015 | Batch 850/1125 | Loss: 0.6856\n",
      "Epoch: 007/015 | Batch 900/1125 | Loss: 0.7591\n",
      "Epoch: 007/015 | Batch 950/1125 | Loss: 0.7277\n",
      "Epoch: 007/015 | Batch 1000/1125 | Loss: 0.6921\n",
      "Epoch: 007/015 | Batch 1050/1125 | Loss: 0.7019\n",
      "Epoch: 007/015 | Batch 1100/1125 | Loss: 0.7508\n",
      "training accuracy: 49.99%\n",
      "valid accuracy: 50.85%\n",
      "Time elapsed: 1.50 min\n",
      "Epoch: 008/015 | Batch 000/1125 | Loss: 0.7085\n",
      "Epoch: 008/015 | Batch 050/1125 | Loss: 0.6474\n",
      "Epoch: 008/015 | Batch 100/1125 | Loss: 0.7000\n",
      "Epoch: 008/015 | Batch 150/1125 | Loss: 0.6901\n",
      "Epoch: 008/015 | Batch 200/1125 | Loss: 0.6928\n",
      "Epoch: 008/015 | Batch 250/1125 | Loss: 0.6785\n",
      "Epoch: 008/015 | Batch 300/1125 | Loss: 0.6900\n",
      "Epoch: 008/015 | Batch 350/1125 | Loss: 0.7481\n",
      "Epoch: 008/015 | Batch 400/1125 | Loss: 0.6821\n",
      "Epoch: 008/015 | Batch 450/1125 | Loss: 0.7159\n",
      "Epoch: 008/015 | Batch 500/1125 | Loss: 0.7027\n",
      "Epoch: 008/015 | Batch 550/1125 | Loss: 0.7232\n",
      "Epoch: 008/015 | Batch 600/1125 | Loss: 0.6871\n",
      "Epoch: 008/015 | Batch 650/1125 | Loss: 0.7168\n",
      "Epoch: 008/015 | Batch 700/1125 | Loss: 0.6230\n",
      "Epoch: 008/015 | Batch 750/1125 | Loss: 0.7069\n",
      "Epoch: 008/015 | Batch 800/1125 | Loss: 0.7106\n",
      "Epoch: 008/015 | Batch 850/1125 | Loss: 0.7879\n",
      "Epoch: 008/015 | Batch 900/1125 | Loss: 0.7593\n",
      "Epoch: 008/015 | Batch 950/1125 | Loss: 0.7644\n",
      "Epoch: 008/015 | Batch 1000/1125 | Loss: 0.6887\n",
      "Epoch: 008/015 | Batch 1050/1125 | Loss: 0.8822\n",
      "Epoch: 008/015 | Batch 1100/1125 | Loss: 0.7760\n",
      "training accuracy: 50.15%\n",
      "valid accuracy: 49.20%\n",
      "Time elapsed: 1.71 min\n",
      "Epoch: 009/015 | Batch 000/1125 | Loss: 0.8811\n",
      "Epoch: 009/015 | Batch 050/1125 | Loss: 0.7075\n",
      "Epoch: 009/015 | Batch 100/1125 | Loss: 0.7588\n",
      "Epoch: 009/015 | Batch 150/1125 | Loss: 0.7640\n",
      "Epoch: 009/015 | Batch 200/1125 | Loss: 0.7381\n",
      "Epoch: 009/015 | Batch 250/1125 | Loss: 0.7755\n",
      "Epoch: 009/015 | Batch 300/1125 | Loss: 1.0857\n",
      "Epoch: 009/015 | Batch 350/1125 | Loss: 0.9904\n",
      "Epoch: 009/015 | Batch 400/1125 | Loss: 0.6867\n",
      "Epoch: 009/015 | Batch 450/1125 | Loss: 0.9991\n",
      "Epoch: 009/015 | Batch 500/1125 | Loss: 0.6407\n",
      "Epoch: 009/015 | Batch 550/1125 | Loss: 1.2088\n",
      "Epoch: 009/015 | Batch 600/1125 | Loss: 0.6897\n",
      "Epoch: 009/015 | Batch 650/1125 | Loss: 0.7668\n",
      "Epoch: 009/015 | Batch 700/1125 | Loss: 0.6836\n",
      "Epoch: 009/015 | Batch 750/1125 | Loss: 0.6990\n",
      "Epoch: 009/015 | Batch 800/1125 | Loss: 0.6487\n",
      "Epoch: 009/015 | Batch 850/1125 | Loss: 0.9443\n",
      "Epoch: 009/015 | Batch 900/1125 | Loss: 0.7636\n",
      "Epoch: 009/015 | Batch 950/1125 | Loss: 1.0837\n",
      "Epoch: 009/015 | Batch 1000/1125 | Loss: 0.8099\n",
      "Epoch: 009/015 | Batch 1050/1125 | Loss: 0.6885\n",
      "Epoch: 009/015 | Batch 1100/1125 | Loss: 0.7187\n",
      "training accuracy: 50.18%\n",
      "valid accuracy: 50.93%\n",
      "Time elapsed: 1.92 min\n",
      "Epoch: 010/015 | Batch 000/1125 | Loss: 0.7861\n",
      "Epoch: 010/015 | Batch 050/1125 | Loss: 1.0210\n",
      "Epoch: 010/015 | Batch 100/1125 | Loss: 0.6744\n",
      "Epoch: 010/015 | Batch 150/1125 | Loss: 0.7190\n",
      "Epoch: 010/015 | Batch 200/1125 | Loss: 0.7450\n",
      "Epoch: 010/015 | Batch 250/1125 | Loss: 0.7238\n",
      "Epoch: 010/015 | Batch 300/1125 | Loss: 0.7091\n",
      "Epoch: 010/015 | Batch 350/1125 | Loss: 0.7408\n",
      "Epoch: 010/015 | Batch 400/1125 | Loss: 0.6972\n",
      "Epoch: 010/015 | Batch 450/1125 | Loss: 0.7892\n",
      "Epoch: 010/015 | Batch 500/1125 | Loss: 0.9912\n",
      "Epoch: 010/015 | Batch 550/1125 | Loss: 1.5418\n",
      "Epoch: 010/015 | Batch 600/1125 | Loss: 1.2797\n",
      "Epoch: 010/015 | Batch 650/1125 | Loss: 0.6903\n",
      "Epoch: 010/015 | Batch 700/1125 | Loss: 0.7377\n",
      "Epoch: 010/015 | Batch 750/1125 | Loss: 0.6951\n",
      "Epoch: 010/015 | Batch 800/1125 | Loss: 0.6916\n",
      "Epoch: 010/015 | Batch 850/1125 | Loss: 0.7673\n",
      "Epoch: 010/015 | Batch 900/1125 | Loss: 0.7079\n",
      "Epoch: 010/015 | Batch 950/1125 | Loss: 0.9698\n",
      "Epoch: 010/015 | Batch 1000/1125 | Loss: 0.8825\n",
      "Epoch: 010/015 | Batch 1050/1125 | Loss: 0.6939\n",
      "Epoch: 010/015 | Batch 1100/1125 | Loss: 0.7192\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[39mprint\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mNUM_EPOCHS\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     23\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_dataloader)\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     24\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     27\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining accuracy: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcompute_accuracy(model,\u001b[39m \u001b[39;49mtrain_dataloader,\u001b[39m \u001b[39;49mDEVICE)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     29\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mvalid accuracy: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     30\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcompute_accuracy(model,\u001b[39m \u001b[39mval_dataloader,\u001b[39m \u001b[39mDEVICE)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTime elapsed: \u001b[39m\u001b[39m{\u001b[39;00m(time\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time)\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal Training Time: \u001b[39m\u001b[39m{\u001b[39;00m(time\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time)\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m         _, predicted_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(logits, \u001b[39m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m         num_examples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m         correct_pred \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted_labels \u001b[39m==\u001b[39;49m targets)\u001b[39m.\u001b[39msum()\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m correct_pred\u001b[39m.\u001b[39mfloat()\u001b[39m/\u001b[39mnum_examples \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_dataloader):\n",
    "        text, labels = batch_data\n",
    "        text = text.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_dataloader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_dataloader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, val_dataloader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_dataloader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_2(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "            print(targets)\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            print(targets, predicted_labels, end='\\n\\n')\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy_2(model, train_dataloader, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
