{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import unidecode\n",
    "import string\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "TEXT_PORTION_SIZE = 200\n",
    "\n",
    "NUM_ITER = 5000\n",
    "LEARNING_RATE = 0.005\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text: 84658\n"
     ]
    }
   ],
   "source": [
    "with open('covid19-faq.txt', 'r') as f:\n",
    "    textfile = f.read()\n",
    "# convert special characters\n",
    "textfile = unidecode.unidecode(textfile)\n",
    "# strip extra whitespaces\n",
    "textfile = re.sub(' +',' ', textfile)\n",
    "TEXT_LENGTH = len(textfile)\n",
    "print(f'Number of characters in text: {TEXT_LENGTH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the text into smaller portions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heck with the vaccine provider.\n",
      "\n",
      "If you're traveling away from Madison before your second dose, we encourage you to get your first dose on campus now and obtain a second dose once you reach your new lo\n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "def random_portion(textfile):\n",
    "    start_index = random.randint(0, TEXT_LENGTH - TEXT_PORTION_SIZE)\n",
    "    end_index = start_index + TEXT_PORTION_SIZE + 1\n",
    "    return textfile[start_index:end_index]\n",
    "\n",
    "print(random_portion(textfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to convert characters into tensors of integers (type long):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "def char_to_tensor(text):\n",
    "    lst = [string.printable.index(c) for c in text]\n",
    "    tensor = torch.tensor(lst).long()\n",
    "    return tensor\n",
    "\n",
    "print(char_to_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it together to make a function that draws random batches for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_sample(textfile):    \n",
    "    text_long = char_to_tensor(random_portion(textfile))\n",
    "    inputs = text_long[:-1]\n",
    "    targets = text_long[1:]\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 94, 24, 27, 94, 21, 10, 29, 14, 27, 94, 24, 15, 94, 26, 30, 10, 27,\n",
       "         10, 23, 29, 18, 23, 14, 94, 10, 23, 13, 94, 34, 24, 30, 27, 94, 37, 10,\n",
       "         13, 16, 14, 27, 94, 37, 10, 13, 16, 14, 94, 18, 23, 13, 18, 12, 10, 29,\n",
       "         14, 28, 94, 63, 11, 30, 18, 21, 13, 18, 23, 16, 94, 10, 12, 12, 14, 28,\n",
       "         28, 94, 16, 27, 10, 23, 29, 14, 13, 75, 63, 96, 54, 14, 14, 94, 29, 17,\n",
       "         14, 94, 56, 23, 18, 31, 14, 27, 28, 18, 29, 34, 94, 43, 14, 10, 21, 29,\n",
       "         17, 94, 54, 14, 27, 31, 18, 12, 14, 28, 94, 32, 14, 11, 28, 18, 29, 14,\n",
       "         94, 15, 24, 27, 94, 22, 24, 27, 14, 94, 18, 23, 15, 24, 27, 22, 10, 29,\n",
       "         18, 24, 23, 94, 10, 11, 24, 30, 29, 94, 26, 30, 10, 27, 10, 23, 29, 18,\n",
       "         23, 14, 94, 10, 23, 13, 94, 18, 28, 24, 21, 10, 29, 18, 24, 23, 75, 96,\n",
       "         96, 58, 17, 18, 21, 14, 94,  1,  4, 94, 13, 10, 34, 28, 94, 18, 28, 94,\n",
       "         29, 17]),\n",
       " tensor([94, 24, 27, 94, 21, 10, 29, 14, 27, 94, 24, 15, 94, 26, 30, 10, 27, 10,\n",
       "         23, 29, 18, 23, 14, 94, 10, 23, 13, 94, 34, 24, 30, 27, 94, 37, 10, 13,\n",
       "         16, 14, 27, 94, 37, 10, 13, 16, 14, 94, 18, 23, 13, 18, 12, 10, 29, 14,\n",
       "         28, 94, 63, 11, 30, 18, 21, 13, 18, 23, 16, 94, 10, 12, 12, 14, 28, 28,\n",
       "         94, 16, 27, 10, 23, 29, 14, 13, 75, 63, 96, 54, 14, 14, 94, 29, 17, 14,\n",
       "         94, 56, 23, 18, 31, 14, 27, 28, 18, 29, 34, 94, 43, 14, 10, 21, 29, 17,\n",
       "         94, 54, 14, 27, 31, 18, 12, 14, 28, 94, 32, 14, 11, 28, 18, 29, 14, 94,\n",
       "         15, 24, 27, 94, 22, 24, 27, 14, 94, 18, 23, 15, 24, 27, 22, 10, 29, 18,\n",
       "         24, 23, 94, 10, 11, 24, 30, 29, 94, 26, 30, 10, 27, 10, 23, 29, 18, 23,\n",
       "         14, 94, 10, 23, 13, 94, 18, 28, 24, 21, 10, 29, 18, 24, 23, 75, 96, 96,\n",
       "         58, 17, 18, 21, 14, 94,  1,  4, 94, 13, 10, 34, 28, 94, 18, 28, 94, 29,\n",
       "         17, 14]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_random_sample(textfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tembed = torch.nn.Embedding(num_embeddings=len(string.printable), embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tembed(torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size        \n",
    "        self.embed = torch.nn.Embedding(num_embeddings=input_size, embedding_dim=embed_size)\n",
    "        self.rnn = torch.nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, character, hidden, cell_state):  # expects character as size [batch_size, 1]           \n",
    "        embedded = self.embed(character) # [batch size, embedding dim] = [1, embedding dim]\n",
    "        (hidden, cell_state) = self.rnn(embedded, (hidden, cell_state))\n",
    "        # 1. output dim: [batch size, output_size] = [1, output_size]\n",
    "        # 2. hidden dim: [batch size, hidden dim] = [1, hidden dim]\n",
    "        # 3. cell dim: [batch size, hidden dim] = [1, hidden dim]\n",
    "        output = self.fc(hidden) # [batch size, output_size] = [1, output_size]\n",
    "        return output, hidden, cell_state\n",
    "      \n",
    "    def init_zero_state(self):\n",
    "        init_hidden = torch.zeros(1, self.hidden_size).to(DEVICE)\n",
    "        init_cell = torch.zeros(1, self.hidden_size).to(DEVICE)\n",
    "        return (init_hidden, init_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(len(string.printable), EMBEDDING_DIM, HIDDEN_DIM, len(string.printable))\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    ## based on https://github.com/spro/practical-pytorch/\n",
    "    ## blob/master/char-rnn-generation/char-rnn-generation.ipynb\n",
    "\n",
    "    (hidden, cell_state) = model.init_zero_state()\n",
    "    prime_input = char_to_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        inp = prime_input[p].unsqueeze(0)\n",
    "        _, hidden, cell_state = model(inp.to(DEVICE), hidden, cell_state)\n",
    "    inp = prime_input[-1].unsqueeze(0)\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "\n",
    "        outputs, hidden, cell_state = model(inp.to(DEVICE), hidden, cell_state)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = outputs.data.view(-1).div(temperature).exp() # e^{logits / T}\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = string.printable[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_to_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.00 min\n",
      "Iteration 0 | Loss 4.49\n",
      "\n",
      "\n",
      "Th(\\T-\\`#XT&fW6BSLm1^GAqrU?w8=kH;,XATEO`+pgDgbuf\f=35kY3jsMJ\\K<@Gh:`|ch1@\u000b%t-D^tj|80\"Y\n",
      "xG),qw(37-Y&plFot{6!*9tO5?E[#M4)!s@2)iRxvC8Wx:\"$\n",
      ",81_T)G]RxymA^8^F(r\"a^nDtu_aGc( \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 0.25 min\n",
      "Iteration 200 | Loss 2.30\n",
      "\n",
      "\n",
      "Th ated thow the fated ince gest reppeis, on the mepters) halyserstives buill th akp bice ind the the be off to their builaling if cproution will abliving of ay to stive the saline semomeilg the on to t \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 0.50 min\n",
      "Iteration 400 | Loss 1.71\n",
      "\n",
      "\n",
      "Therivif does you be preen livate. bation and ampustal off-cand tests rocal will or appose vaccing where thes sith this the and varal their not munitins bating to be paring we the need Wacing youres as  \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 0.74 min\n",
      "Iteration 600 | Loss 1.37\n",
      "\n",
      "\n",
      "The whis is that to suby not requires a spedontaks, and to more to shonsif useral access to provided to varming be that us to campus reasome or are app Aploy not frome prole to the COVID-19 Safer for st \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 0.99 min\n",
      "Iteration 800 | Loss 1.55\n",
      "\n",
      "\n",
      "Thit and used inter who endicitine contain and and in and on the vaccine regerson date. A supto be in in the We resurent in are recormagaing from and reder all and reete their vicated cond and you at co \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 1.24 min\n",
      "Iteration 1000 | Loss 1.19\n",
      "\n",
      "\n",
      "Thes when to provider.)\n",
      "\n",
      "Griduring servicels.\n",
      "\n",
      "Daccinal your mout the where who on the status in to need to the tested maanates, badge be as those not mean being app to an eek of The and past campus gra \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 1.49 min\n",
      "Iteration 1200 | Loss 1.42\n",
      "\n",
      "\n",
      "The a Union as comple in fiminled buildinate Safer Badger Badger an cellompl campus are facilition on chersonal as or the Lividenrompted vacciness are positive busine a positive test but or campus (work \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 1.74 min\n",
      "Iteration 1400 | Loss 1.51\n",
      "\n",
      "\n",
      "The Safer for positive and event of the Safer Badge will need to do native a remaibility expusion test within Univer the well request selic from and accertions to encervass. Arl and when indiversion.\n",
      "\n",
      "W \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 1.99 min\n",
      "Iteration 1600 | Loss 1.11\n",
      "\n",
      "\n",
      "The users, sharmation of a dose you may geturn to the positive within vaccination on week retors of the funtive help you who enwill pacies.\n",
      "\n",
      "Services wbith agente in that will not the monitors notificat \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 2.26 min\n",
      "Iteration 1800 | Loss 1.29\n",
      "\n",
      "\n",
      "The appointmentation same and participate at result at they will need and are parly and overs and safed turn is access shorlare positities are come not miture formacy, participants of the testing access \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 2.53 min\n",
      "Iteration 2000 | Loss 0.89\n",
      "\n",
      "\n",
      "The app scolled the Clowe the ressure phosites dose required by a the 9wind will be also buildity to wer they contain a negative active test to plare stidation to prevent on as is a test further symptom \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 2.79 min\n",
      "Iteration 2200 | Loss 1.07\n",
      "\n",
      "\n",
      "Thin for COVID-19 addation phoness provide and is make requirements for vaccine it receive tests.\n",
      "\n",
      "Results and contacked care protoce addition.\n",
      "\n",
      "The campus will remain a receative for semesters in proco \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 3.05 min\n",
      "Iteration 2400 | Loss 1.25\n",
      "\n",
      "\n",
      "Thins with a regiints in the neagly Health 102--our required two and for information of result. Eve some assor 10 days provider in the campus test in sedentmine you modany a green campus workings access \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 3.30 min\n",
      "Iteration 2600 | Loss 0.96\n",
      "\n",
      "\n",
      "These with other has as a comple able and.\n",
      "\n",
      "Yes, a untly dose in information available and follow \"need and vaccine enter the Safer Badgers app a vaccinat semested inforal heasable to access to test sit \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 3.55 min\n",
      "Iteration 2800 | Loss 0.99\n",
      "\n",
      "\n",
      "This your may be results and insurages will guet stall after. If your results tracks for saliva-isord to private to Cl sident, be supervisoved to tested building your result to estion will be coverings  \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 3.81 min\n",
      "Iteration 3000 | Loss 1.55\n",
      "\n",
      "\n",
      "The lassible, health employees also disconsings will clear have such as lesteristance public the shot we entered to the Badger Badge Disability and our remote in the follow campus campus get care being  \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 4.06 min\n",
      "Iteration 3200 | Loss 1.24\n",
      "\n",
      "\n",
      "The PDF. Bectudent, Medical to chein card at 608-262-5607 Wisconsin Department ontmine with wat health progrmation symptoms some location having than the see community and secus cause the evening state  \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 4.32 min\n",
      "Iteration 3400 | Loss 0.91\n",
      "\n",
      "\n",
      "The Wisconsin Welllquent they secentining and covering of the Union (seleater possible these results and are officiation with the virus they are not limited to the virus regular are. Visity Health Servi \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 4.57 min\n",
      "Iteration 3600 | Loss 0.87\n",
      "\n",
      "\n",
      "Tho turn to be all test on campus, you wiv. Using testing program that one may be regular contial encourager Badge with their changes and may be participation student receive (you may complete federal f \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 4.82 min\n",
      "Iteration 3800 | Loss 1.10\n",
      "\n",
      "\n",
      "Thing normal governal Chened at or realts regular to requirement of the testing proximility Recreation require you may be experiated to any appearing that changed at COVID-19, you have be sure the class \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 5.07 min\n",
      "Iteration 4000 | Loss 1.16\n",
      "\n",
      "\n",
      "The tokens and when you an implec meminitivining saliva reasons to a conding brility allow short partries is not be an if the essout to the test. Your exemption will continue any as policies can not the \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 5.32 min\n",
      "Iteration 4200 | Loss 1.54\n",
      "\n",
      "\n",
      "Theinittent without from two negative test sinlution of fill be and academic seleval Close in-person University Health Services to attents into the earned a so have a tendations for exemption to campus  \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 5.57 min\n",
      "Iteration 4400 | Loss 1.14\n",
      "\n",
      "\n",
      "This of campus regardless will need to the use for campus tested to access deniranting the employees for safety or saliva PCR-based without experiad of test tests with discals to issages requiremed stat \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 5.84 min\n",
      "Iteration 4600 | Loss 0.84\n",
      "\n",
      "\n",
      "The recoed program their normal employees and in University to campus as exemption, including weeks to be an app and campus tested should medical conterng will guents and isolate your qualify all need t \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 6.10 min\n",
      "Iteration 4800 | Loss 1.02\n",
      "\n",
      "\n",
      "The fuction (PII) isolation instructors of vaccine exposure notified as be exposure notification within that you receive as onlect on they retors process track or tap indicating students result as work  \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for iteration in range(NUM_ITER):\n",
    "\n",
    "    hidden, cell_state = model.init_zero_state()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0.\n",
    "    inputs, targets = draw_random_sample(textfile)\n",
    "    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "    \n",
    "    for c in range(TEXT_PORTION_SIZE):\n",
    "        outputs, hidden, cell_state = model(inputs[c].unsqueeze(0), hidden, cell_state)  # inputs[c] = [1] -> inputs[c].unsqueeze(0) = [1, 1]\n",
    "        loss += torch.nn.functional.cross_entropy(outputs, targets[c].view(1))\n",
    "\n",
    "    loss /= TEXT_PORTION_SIZE\n",
    "    loss.backward()\n",
    "    \n",
    "    ### UPDATE MODEL PARAMETERS\n",
    "    optimizer.step()\n",
    "\n",
    "    ### LOGGING\n",
    "    with torch.no_grad():\n",
    "        if iteration % 200 == 0:\n",
    "            print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "            print(f'Iteration {iteration} | Loss {loss.item():.2f}\\n\\n')\n",
    "            print(evaluate(model, 'Th', 200), '\\n')\n",
    "            print(50*'=')\n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "#             plt.clf()\n",
    "#             plt.plot(range(len(loss_list)), loss_list)\n",
    "#             plt.ylabel('Loss')\n",
    "#             plt.xlabel('Iteration x 1000')\n",
    "#             plt.savefig('loss1.pdf')\n",
    "            \n",
    "# plt.clf()\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Iteration x 1000')\n",
    "# plt.plot(range(len(loss_list)), loss_list)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
